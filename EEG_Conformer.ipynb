{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnE4xXxQQChK",
        "outputId": "f7939e2a-9c1a-4ae3-fb39-18dc883d111f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchsummary\n",
        "!pip install torchvision\n",
        "!pip install scipy\n",
        "!pip install einops\n",
        "!pip install transformers\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "yeaqkIl0JvJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e04d61-257c-4b94-f3c4-d18fd98254d6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "gpus = [0]\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
        "import numpy as np\n",
        "import math\n",
        "import scipy.io\n",
        "import random\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import torch.autograd as autograd\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "# from common_spatial_pattern import csp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "\n",
        "# writer = SummaryWriter('./TensorBoardX/')\n",
        "\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io\n",
        "\n",
        "from torch.nn import MSELoss"
      ],
      "metadata": {
        "id": "Z9acubYRJTl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Model**"
      ],
      "metadata": {
        "id": "xk5Xe7q6JVY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy1S6tVpJfUQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e77e05a-e9dd-4df9-8b28-e5042aed9ef1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12412' max='26000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12412/26000 27:04 < 29:38, 7.64 it/s, Epoch 477.35/1000]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>52.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>40.886200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>35.373900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>29.864200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>23.015700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>15.468700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>8.734800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>5.781900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>4.755100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.686700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>4.537200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>4.809100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>4.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>4.610200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.855300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>4.875200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>4.600500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>4.698200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>4.681600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>4.922300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>4.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>4.750400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>4.717100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>4.776400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>4.839400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>4.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>4.731400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>4.857000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>11.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>4.763000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>4.804800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.916800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>4.545900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>4.986600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>4.532500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>4.667100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.768800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>4.728600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>4.697800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>4.582000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>4.900800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>4.711300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>4.852900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>4.760900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>4.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>4.547300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>4.791300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>4.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>4.849500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.732200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>4.571600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>4.661400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>4.711000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>4.726100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.597900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>4.564700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>4.873900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>4.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>4.671200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.759800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>4.643000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>4.706700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>4.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>4.693800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.550900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>4.846600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>4.691500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>4.458300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>4.901500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.646000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>4.748700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>4.716100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>4.755000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>4.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.601500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>4.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>4.598500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>4.651400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>4.745000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.609000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>4.634700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>4.811600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>4.510500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>4.707900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>4.618800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>4.669900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>4.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>4.680400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>4.667500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>4.726100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>4.705300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>4.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>4.700700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>4.660400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>4.719700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>4.422400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>4.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>4.528700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>4.564800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>4.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>4.389500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>4.834800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>4.448500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>4.554600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>4.573800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11300</td>\n",
              "      <td>4.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>4.795700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>4.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>4.572600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>4.561300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11800</td>\n",
              "      <td>4.517800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>4.617500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>4.535800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12100</td>\n",
              "      <td>4.531100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12200</td>\n",
              "      <td>4.566900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>4.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>4.725300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ff03d827bf38>\u001b[0m in \u001b[0;36m<cell line: 297>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1860\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2206\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2209\u001b[0m                 ):\n\u001b[1;32m   2210\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "EEG Conformer\n",
        "\n",
        "Convolutional Transformer for EEG decoding\n",
        "\n",
        "Couple CNN and Transformer in a concise manner with amazing results\n",
        "\"\"\"\n",
        "# remember to change paths\n",
        "\n",
        "\n",
        "\n",
        "# Convolution module\n",
        "# use conv to capture local features, instead of postion embedding.\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, emb_size=40):\n",
        "        # self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "\n",
        "        self.shallownet = nn.Sequential(\n",
        "            nn.Conv2d(4, 40, (2, 2), (1, 1)),\n",
        "            nn.Conv2d(40, 40, (22, 1), (1, 1)),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ELU(),\n",
        "            # nn.AvgPool2d((1,75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
        "            nn.AvgPool2d((1, 3), (1, 1)),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        #print(\"input shape = \",x.shape)\n",
        "        # b, _, _ = x.shape\n",
        "        #print(x)\n",
        "        x = self.shallownet(x)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 num_heads=10,\n",
        "                 drop_p=0.5,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size,b):\n",
        "        super().__init__()\n",
        "        self.regression_head = nn.Sequential(\n",
        "                Reduce('b n e -> b e', reduction='mean'),\n",
        "                nn.LayerNorm(emb_size),\n",
        "                nn.Linear(emb_size, b)  # Output a single value for regression\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.regression_head(x)\n",
        "        return out  # No need to return x, out\n",
        "\n",
        "class Conformer(nn.Module):\n",
        "    def __init__(self, batch = 4,emb_size=40, depth=6, n_classes=4, **kwargs):\n",
        "        super(Conformer, self).__init__()\n",
        "        self.patch_embedding = PatchEmbedding(emb_size)\n",
        "        self.transformer_encoder = TransformerEncoder(depth, emb_size)\n",
        "        self.classification_head = ClassificationHead(emb_size,batch)\n",
        "\n",
        "    def forward(self, input, label=None):\n",
        "        # print(\"input.shape = \", input.shape)\n",
        "        input = input[None, :, :, :]\n",
        "        x = self.patch_embedding(input)\n",
        "        x = self.transformer_encoder(x)\n",
        "        # print(\"Shape before classification head:\", x.shape)  # Debugging line\n",
        "        x = self.classification_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "def pad_collate(batch):\n",
        "    # Extract inputs and labels from the batch\n",
        "    inputs = [item['input'] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "\n",
        "    # Check if the batch size is not a multiple of 4\n",
        "    required_batch_size = batch_size\n",
        "    shortfall = len(batch) % required_batch_size\n",
        "    if shortfall > 0:\n",
        "        # Calculate how many samples to add\n",
        "        samples_to_add = required_batch_size - shortfall\n",
        "        # Randomly select samples to add\n",
        "        for _ in range(samples_to_add):\n",
        "            random_sample = random.choice(batch)  # Assuming 'random' is already imported\n",
        "            inputs.append(random_sample['input'])\n",
        "            labels.append(random_sample['label'])\n",
        "\n",
        "    # Pad the inputs to have the same length\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Stack labels into a single tensor\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return {'input': inputs_padded, 'label': labels}\n",
        "\n",
        "\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, root_dir, max_timesteps=1000):\n",
        "        self.max_timesteps = max_timesteps\n",
        "        self.data_files = []\n",
        "        self.portion_counts = []  # Store the number of portions per file\n",
        "        self.labels = []\n",
        "        for folder_name in os.listdir(root_dir):\n",
        "            if '(' in folder_name and ')' in folder_name:\n",
        "                label = int(folder_name.split('(')[-1].split(')')[0])\n",
        "                folder_path = os.path.join(root_dir, folder_name)\n",
        "                for file_name in os.listdir(folder_path):\n",
        "                    if file_name.endswith('.mat') and file_name != 'FFT.mat':\n",
        "                        file_path = os.path.join(folder_path, file_name)\n",
        "                        self.data_files.append(file_path)\n",
        "                        self.labels.append(label)\n",
        "                        # Determine how many portions this file will be split into\n",
        "                        data = scipy.io.loadmat(file_path)['data']\n",
        "                        portions = math.ceil(data.shape[1] / max_timesteps)\n",
        "                        self.portion_counts.append(portions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(self.portion_counts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find which file and which portion this index corresponds to\n",
        "        file_idx = 0\n",
        "        while idx >= self.portion_counts[file_idx]:\n",
        "            idx -= self.portion_counts[file_idx]\n",
        "            file_idx += 1\n",
        "        portion_idx = idx\n",
        "        label = self.labels[file_idx]\n",
        "        data_path = self.data_files[file_idx]\n",
        "        data = scipy.io.loadmat(data_path)['data']\n",
        "        # Calculate the start and end indices for this portion\n",
        "        start_idx = portion_idx * self.max_timesteps\n",
        "        end_idx = min((portion_idx + 1) * self.max_timesteps, data.shape[1])\n",
        "        # Slice the data for this portion\n",
        "        data_portion = data[:, start_idx:end_idx]\n",
        "        data_portion = torch.tensor(data_portion, dtype=torch.float32)\n",
        "        return {'input': data_portion, 'label_ids': label, 'label': torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "def model_init():\n",
        "    return Conformer()  # Initialize your modified Conformer model here\n",
        "\n",
        "\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_fn = MSELoss()\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs[\"label\"].float()  # Ensure labels are float for MSE Loss\n",
        "        outputs = model(**inputs)\n",
        "        # Reshape labels to match output shape (batch_size, 1)\n",
        "        labels = labels.view(-1, 1)\n",
        "        labels = labels.transpose(0, 1)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1000,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-3,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    #save_total_limit=3,\n",
        ")\n",
        "\n",
        "data_collator = pad_collate\n",
        "\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=EEGDataset(root_dir='/content/drive/MyDrive/be_lab/data/G01_data_cut'),\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model_path = './trained_conformer_model.pth'\n",
        "torch.save(trainer.model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "IcwOuoGYEHC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1aec1f-9b44-4fb0-bb16-3091d7aab30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./trained_conformer_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_inference():\n",
        "    # Initialize the model\n",
        "    model = model_init()\n",
        "    # Load the trained model weights\n",
        "    model.load_state_dict(torch.load(\"/content/trained_conformer_model.pth\"))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # DataLoader for inference dataset\n",
        "    inference_dataset = EEGDataset(root_dir='/content/drive/MyDrive/be_lab/data/G01_data_cut')  # Use the provided data directory\n",
        "    inference_loader = DataLoader(inference_dataset, batch_size=1, collate_fn=pad_collate)\n",
        "\n",
        "    # Check the size of the dataset\n",
        "    print(f\"Dataset size: {len(inference_dataset)}\")\n",
        "\n",
        "    # Try to get the first item from the dataset\n",
        "    if len(inference_dataset) > 0:\n",
        "        first_item = inference_dataset[0]\n",
        "        print(f\"First item keys: {first_item.keys()}\")\n",
        "    else:\n",
        "        print(\"Dataset is empty. Check the dataset path and contents.\")\n",
        "\n",
        "    # correct_predictions = 0\n",
        "    # total_predictions = 0\n",
        "    total_error = 0\n",
        "    batches = 0\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for batch in inference_loader:\n",
        "            batches += 1\n",
        "            inputs = batch['input']\n",
        "            true_labels = batch['label']  # Assuming you want to compare against true labels\n",
        "            outputs = model(inputs)\n",
        "            # print(inputs)\n",
        "            print(outputs)\n",
        "            print(true_labels)\n",
        "            #true_label = true_labels.transpose(0, 1)\n",
        "            # print(true_label)\n",
        "            # _, predicted_labels = torch.max(outputs, 1)\n",
        "            predicted_labels = outputs\n",
        "            total_error += abs(predicted_labels - true_labels).sum()\n",
        "            # print(predicted_labels, true_labels\n",
        "            # correct_predictions += (predicted_labels == true_labels).sum().item()\n",
        "            # total_predictions += true_labels.size(0)\n",
        "            # Process the outputs as needed\n",
        "            #print(outputs)\n",
        "    # accuracy = correct_predictions / total_predictions\n",
        "    # print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(\"average_error = \", total_error / batches)\n",
        "    print(\"batches = \", batches)\n",
        "    print(\"total_error = \", total_error)\n",
        "    print(\"len of dataset = \", len(inference_dataset))\n",
        "\n",
        "perform_inference()\n",
        "\n"
      ],
      "metadata": {
        "id": "oC0XSKsdFy_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a93d49d-c9b9-4671-d30e-2fbf55486090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 103\n",
            "First item keys: dict_keys(['input', 'label_ids', 'label'])\n",
            "tensor([[9.6928, 7.2218, 7.3124, 7.4642]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[9.4314, 7.6260, 7.6931, 7.8083]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.3761, 8.0902, 8.0881, 8.1228]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.6492, 8.0383, 8.0531, 8.1041]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[4.4689, 7.2810, 7.1072, 7.0110]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[9.5410, 7.4924, 7.5389, 7.7073]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.4441, 8.0914, 8.0883, 8.1382]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.2870, 8.0791, 8.0772, 8.1012]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[6.2735, 7.8982, 7.7887, 7.7632]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.4300, 8.0948, 8.0896, 8.1424]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.8687, 7.9895, 7.9990, 8.0928]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[6.7142, 7.9817, 7.9123, 7.8666]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[7.3173, 8.0974, 8.0375, 8.0359]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[5.7062, 7.7351, 7.6034, 7.5429]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[7.9883, 8.1305, 8.0981, 8.1409]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[8.7490, 7.4083, 7.5558, 7.6429]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[5.9833, 7.8194, 7.7010, 7.6516]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[6.6539, 7.9830, 7.8976, 7.8735]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[5.4207, 7.6240, 7.5095, 7.4103]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[7.7545, 8.1284, 8.0896, 8.1041]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[9.1674, 7.7707, 7.8466, 7.9144]])\n",
            "tensor([8, 8, 8, 8])\n",
            "tensor([[6.0909, 7.8271, 7.7374, 7.6605]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[7.2959, 8.0828, 8.0229, 8.0115]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.2880, 7.7273, 7.7970, 7.8838]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.1947, 7.8303, 7.8659, 7.9743]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.7131, 7.9810, 8.0140, 8.0545]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.7597, 8.0077, 8.0318, 8.0876]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[7.7165, 8.0945, 8.0706, 8.0715]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[5.4429, 7.6149, 7.5113, 7.3961]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.3812, 8.0801, 8.0825, 8.1146]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[7.0066, 8.0500, 7.9753, 7.9529]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[7.5930, 8.1211, 8.0727, 8.0774]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[7.6656, 8.1135, 8.0603, 8.1153]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[8.1881, 8.1215, 8.1028, 8.1484]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[8.5335, 8.0740, 8.0592, 8.1473]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[6.2787, 7.8940, 7.7932, 7.7426]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[8.6705, 8.0394, 8.0532, 8.1100]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[7.3926, 7.8020, 7.8589, 7.7720]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[9.1838, 7.8237, 7.8415, 7.9822]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[5.4398, 7.6537, 7.5041, 7.4570]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[7.5520, 8.0916, 8.0530, 8.0448]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[9.1336, 7.8455, 7.8569, 8.0013]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[9.7854, 6.5687, 6.7050, 6.8739]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[7.6931, 8.1290, 8.0843, 8.0968]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[7.2522, 8.0127, 7.9955, 7.9444]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[9.0584, 7.8158, 7.8930, 7.9511]])\n",
            "tensor([5, 5, 5, 5])\n",
            "tensor([[9.1065, 7.7907, 7.8658, 7.9283]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[6.1752, 7.7444, 7.7092, 7.6136]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[5.4463, 7.6332, 7.5174, 7.4215]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[9.1814, 7.8091, 7.8675, 7.9508]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[7.0744, 7.9868, 7.9658, 7.9038]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[4.4565, 7.2631, 7.1091, 6.9896]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[8.3904, 8.0643, 8.0678, 8.0925]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[8.8110, 7.8818, 7.9514, 7.9794]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[8.7408, 7.8864, 7.9614, 7.9924]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[9.0292, 7.7704, 7.8606, 7.9131]])\n",
            "tensor([7, 7, 7, 7])\n",
            "tensor([[9.7889, 6.4512, 6.5816, 6.7694]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[5.0175, 7.4424, 7.3389, 7.2000]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.7181, 7.0006, 7.1305, 7.2631]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.3663, 7.4729, 7.6002, 7.6671]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.9944, 7.7855, 7.8743, 7.9079]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.7867, 6.8338, 6.9392, 7.1195]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.7582, 6.5190, 6.6752, 6.8257]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[5.5224, 7.6255, 7.5323, 7.4081]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[7.5833, 8.0784, 8.0563, 8.0449]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.4479, 7.5924, 7.6712, 7.7817]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.7893, 6.4711, 6.6031, 6.7883]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.4968, 7.5555, 7.6262, 7.7526]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.7851, 6.6851, 6.8175, 6.9834]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.5819, 7.4385, 7.5162, 7.6484]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[7.4954, 8.0968, 8.0513, 8.0475]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[9.7616, 6.9645, 7.0736, 7.2328]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[9.3158, 7.6421, 7.7364, 7.8131]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[8.0628, 8.1215, 8.0979, 8.1192]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[5.4209, 7.5961, 7.5008, 7.3796]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[8.3529, 8.0758, 8.0764, 8.1053]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[8.0603, 8.1162, 8.0978, 8.1214]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[8.7935, 7.9721, 8.0079, 8.0569]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[7.9585, 7.7865, 7.8735, 7.8805]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[8.6126, 8.0410, 8.0591, 8.1091]])\n",
            "tensor([9, 9, 9, 9])\n",
            "tensor([[8.7941, 7.9712, 8.0059, 8.0557]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[5.9369, 7.8035, 7.6867, 7.6293]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.8384, 7.9955, 8.0151, 8.0910]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.1247, 8.1124, 8.0966, 8.1202]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[6.6968, 7.9641, 7.9005, 7.8401]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[3.0232, 6.5750, 6.3924, 6.2332]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[4.4849, 7.2845, 7.1171, 7.0108]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[3.8791, 7.0101, 6.8352, 6.7075]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.3287, 7.7158, 7.7772, 7.8773]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[4.9385, 7.4357, 7.3152, 7.1866]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[8.0045, 8.1002, 8.0832, 8.0953]])\n",
            "tensor([10, 10, 10, 10])\n",
            "tensor([[9.1517, 7.8587, 7.8838, 7.9980]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[7.8014, 8.1186, 8.0717, 8.1321]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[4.1981, 7.1134, 6.9832, 6.8320]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[4.8061, 7.4201, 7.2517, 7.1700]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[9.5300, 7.4580, 7.4960, 7.6863]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[7.3461, 8.0946, 8.0405, 8.0436]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[8.6709, 8.0407, 8.0545, 8.1157]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[4.6296, 7.3469, 7.1774, 7.0909]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[8.8452, 7.9730, 8.0072, 8.0666]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[8.5164, 8.0426, 8.0547, 8.0866]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[8.2714, 8.1173, 8.1000, 8.1469]])\n",
            "tensor([4, 4, 4, 4])\n",
            "tensor([[8.8860, 7.9597, 7.9963, 8.0657]])\n",
            "tensor([4, 4, 4, 4])\n",
            "average_error =  tensor(7.8039)\n",
            "batches =  103\n",
            "total_error =  tensor(803.7975)\n",
            "len of dataset =  103\n"
          ]
        }
      ]
    }
  ]
}